{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a92c7d01",
   "metadata": {},
   "source": [
    "# Part 1: Sequential Conversation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc9b726",
   "metadata": {},
   "source": [
    "### Imports & API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03051aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder, ChatPromptTemplate\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import BaseMessage, SystemMessage\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.runnables import ConfigurableFieldSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27f71d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# below should not be changed\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "# you can change this as preferred\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"chatbot_agent\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de041aea",
   "metadata": {},
   "source": [
    "### Initialize LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87b50576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For normal accurate responses\n",
    "llm = ChatOpenAI(temperature=0.0, model=\"gpt-4.1-nano\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96ff964",
   "metadata": {},
   "source": [
    "### Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f2919d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a helpful assistant called Alex.\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{query}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c230de35",
   "metadata": {},
   "source": [
    "### Chatbot Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "590c95e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = prompt_template | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439b99cd",
   "metadata": {},
   "source": [
    "### ConversationSummaryBufferMemory\n",
    "Based on number of messages. Where if number of messages is more than k, pop oldest messages and create a new summary by adding information from poped messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "90f9ee0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationSummaryBufferMemory_custom(BaseChatMessageHistory, BaseModel):\n",
    "    messages: list[BaseMessage] = Field(default_factory=list)\n",
    "    k: int = Field(default_factory=int)\n",
    "\n",
    "    def __init__(self, k: int):\n",
    "        super().__init__(k=k)\n",
    "        print(f\"Initializing ConversationSummaryBufferMemory_custom with k={k}\")\n",
    "\n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        \"\"\"\n",
    "        Add messages to the history, removing any messages beyond the last 'k' messages.\n",
    "        \"\"\"\n",
    "        self.messages.extend(messages)\n",
    "        self.messages = self.messages[-self.k:]\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear the history.\"\"\"\n",
    "        self.messages = []\n",
    "\n",
    "# initialize memory\n",
    "chat_map = {}\n",
    "\n",
    "# function to get memory for specific session id\n",
    "def get_chat_history(session_id: str, k: int = 4) -> ConversationSummaryBufferMemory_custom:\n",
    "    print(f\"get_chat_history called with session_id={session_id} and k={k}\")\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = ConversationSummaryBufferMemory_custom(k=k)\n",
    "    # remove anything beyond the last\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cead5dd",
   "metadata": {},
   "source": [
    "### RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "52d1447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key = \"query\",\n",
    "    history_messages_key= \"chat_history\",\n",
    "    history_factory_config= [\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Session ID\",\n",
    "            description=\"The session ID to use for the chat history\",\n",
    "            default=\"id_default\",\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"k\",\n",
    "            annotation=int,\n",
    "            name=\"k\",\n",
    "            description=\"The number of messages to keep in the history\",\n",
    "            default=10,\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993d85f7",
   "metadata": {},
   "source": [
    "### Test LLM Chat with history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0783f48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session_id=id_k10 and k=10\n",
      "Initializing ConversationSummaryBufferMemory_custom with k=10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello, James! Nice to meet you. How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 25, 'total_tokens': 41, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_950f36939b', 'finish_reason': 'stop', 'logprobs': None}, id='run--35894b97-d6a2-4914-8e87-958ff658ba2b-0', usage_metadata={'input_tokens': 25, 'output_tokens': 16, 'total_tokens': 41, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is James\"},\n",
    "    config={\"configurable\": {\"session_id\": \"id_k10\", \"k\": 10}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ce13f565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"I'm an AI model called Alex.\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"id_k10\"].clear()  # clear the history\n",
    "\n",
    "# manually insert history\n",
    "chat_map[\"id_k10\"].add_user_message(\"Hi, my name is James\")\n",
    "chat_map[\"id_k10\"].add_ai_message(\"I'm an AI model called Alex.\")\n",
    "chat_map[\"id_k10\"].add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "chat_map[\"id_k10\"].add_ai_message(\"That's interesting, what are some examples?\")\n",
    "chat_map[\"id_k10\"].add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "chat_map[\"id_k10\"].add_ai_message(\"That's interesting, what's the difference?\")\n",
    "chat_map[\"id_k10\"].add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "chat_map[\"id_k10\"].add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
    "chat_map[\"id_k10\"].add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "chat_map[\"id_k10\"].add_ai_message(\"Very cool!\")\n",
    "\n",
    "chat_map[\"id_k10\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "742b2ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session_id=id_k10 and k=10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is James.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 152, 'total_tokens': 157, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_1a97b5aa6c', 'finish_reason': 'stop', 'logprobs': None}, id='run--00675b68-efbc-4d81-8132-1bb616d28772-0', usage_metadata={'input_tokens': 152, 'output_tokens': 5, 'total_tokens': 157, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"what is my name again?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"id_k10\", \"k\": 10}}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep_Learning_V2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
