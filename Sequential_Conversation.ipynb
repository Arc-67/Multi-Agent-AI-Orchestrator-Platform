{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a92c7d01",
   "metadata": {},
   "source": [
    "# Part 1: Sequential Conversation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc9b726",
   "metadata": {},
   "source": [
    "### Imports & API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03051aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder, ChatPromptTemplate\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import BaseMessage, SystemMessage\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.runnables import ConfigurableFieldSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27f71d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# below should not be changed\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "# you can change this as preferred\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"chatbot_agent\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de041aea",
   "metadata": {},
   "source": [
    "### Initialize LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87b50576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For temperature=0 for normal accurate responses\n",
    "llm = ChatOpenAI(temperature=0.0, model=\"gpt-4.1-nano\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96ff964",
   "metadata": {},
   "source": [
    "### Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f2919d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a conversational AI assistant that uses the conversation history and tool outputs as your only sources of truth.\n",
    "Guidelines:\n",
    "1. Answer only using information from the chat history or tool results - never invent or assume facts.\n",
    "2. If key information is missing, ask one short clarifying question instead of guessing\n",
    "3. If the user changes topic or interrupts, handle it naturally - answer the new query, but retain memory of previous context for when they return\n",
    "4. Be concise, factual, and context-aware. Avoid repetition or over-explanation\n",
    "5. When resuming after an interruption, reuse remembered context if its relevant\n",
    "Your goal is to respond clearly and naturally while maintaining accurate continuity across turns.\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{query}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c230de35",
   "metadata": {},
   "source": [
    "### Chatbot Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "590c95e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = prompt_template | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439b99cd",
   "metadata": {},
   "source": [
    "### ConversationSummaryBufferMemory\n",
    "Based on number of messages. Where if number of messages is more than k, pop oldest messages and create a new summary by adding information from poped messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90f9ee0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationSummaryBufferMemory_custom(BaseChatMessageHistory, BaseModel):\n",
    "    messages: list[BaseMessage] = Field(default_factory=list)\n",
    "    llm: ChatOpenAI = Field(default_factory=ChatOpenAI)\n",
    "    k: int = Field(default_factory=int)\n",
    "\n",
    "    def __init__(self, llm: ChatOpenAI, k: int):\n",
    "        super().__init__(llm=llm, k=k)\n",
    "        # print(f\"Initializing ConversationSummaryBufferMemory_custom with k={k}\")\n",
    "\n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        \"\"\"Add messages to the history, \n",
    "        keep only the last 'k' messages and \n",
    "        generate new summary by combining information from dropped messages.\n",
    "        \"\"\"\n",
    "        existing_summary: SystemMessage | None = None\n",
    "        old_messages: list[BaseMessage] | None = None\n",
    "\n",
    "        # check if there is already a summary message\n",
    "        if len(self.messages) > 0 and isinstance(self.messages[0], SystemMessage):\n",
    "            print(\">> Found existing summary\")\n",
    "            existing_summary = self.messages.pop(0) # remove old summary from messages\n",
    "\n",
    "        # add the new messages to the history\n",
    "        self.messages.extend(messages)\n",
    "\n",
    "        # check if there is too many messages\n",
    "        if len(self.messages) > self.k:\n",
    "            print(\n",
    "                f\">> Found {len(self.messages)} messages, dropping \"\n",
    "                f\"oldest {len(self.messages) - self.k} messages.\")\n",
    "            \n",
    "            # pull out the oldest messages\n",
    "            num_to_drop = len(self.messages) - self.k\n",
    "            old_messages = self.messages[:num_to_drop] # self.messages[:self.k]\n",
    "\n",
    "            # keep only the most recent messages\n",
    "            self.messages = self.messages[-self.k:]\n",
    "\n",
    "        # if no old_messages, no new info to update the summary\n",
    "        if old_messages is None:\n",
    "            print(\">> No old messages to update summary with\")\n",
    "            return\n",
    "        \n",
    "        # construct the summary chat messages\n",
    "        summary_prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(\n",
    "                \"Given the existing conversation summary and the new messages, \"\n",
    "                \"generate a new summary of the conversation. Ensuring to maintain \"\n",
    "                \"as much relevant information as possible.\"\n",
    "            ),\n",
    "            HumanMessagePromptTemplate.from_template(\n",
    "                \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
    "                \"New messages:\\n{old_messages}\"\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        # format the messages and invoke the LLM\n",
    "        new_summary = self.llm.invoke(\n",
    "            summary_prompt.format_messages(\n",
    "                existing_summary=existing_summary,\n",
    "                old_messages=old_messages\n",
    "            )\n",
    "        )\n",
    "\n",
    "        print(f\">> New summary: {new_summary.content}\")\n",
    "        # prepend the new summary to the history\n",
    "        self.messages = [SystemMessage(content=new_summary.content)] + self.messages\n",
    "\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear the history.\"\"\"\n",
    "        self.messages = []\n",
    "\n",
    "# initialize memory\n",
    "chat_map = {}\n",
    "\n",
    "# function to get memory for specific session id\n",
    "def get_chat_history(session_id: str, llm: ChatOpenAI, k: int = 4) -> ConversationSummaryBufferMemory_custom:\n",
    "    print(f\"get_chat_history called with session_id={session_id} and k={k}\")\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = ConversationSummaryBufferMemory_custom(llm=llm, k=k)\n",
    "    # remove anything beyond the last\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cead5dd",
   "metadata": {},
   "source": [
    "### RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52d1447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key = \"query\",\n",
    "    history_messages_key= \"chat_history\",\n",
    "    history_factory_config= [\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Session ID\",\n",
    "            description=\"The session ID to use for the chat history\",\n",
    "            default=\"id_default\",\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"llm\",\n",
    "            annotation=ChatOpenAI,\n",
    "            name=\"LLM\",\n",
    "            description=\"The LLM to use for the conversation summary\",\n",
    "            default=llm,\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"k\",\n",
    "            annotation=int,\n",
    "            name=\"k\",\n",
    "            description=\"The number of messages to keep in the history\",\n",
    "            default=6,\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993d85f7",
   "metadata": {},
   "source": [
    "### Test LLM Chat with history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86d4bd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Message 1\n",
      "---\n",
      "\n",
      "get_chat_history called with session_id=id_k6 and k=6\n",
      ">> No old messages to update summary with\n",
      "---\n",
      "Message 2\n",
      "---\n",
      "\n",
      "get_chat_history called with session_id=id_k6 and k=6\n",
      ">> No old messages to update summary with\n",
      "---\n",
      "Message 3\n",
      "---\n",
      "\n",
      "get_chat_history called with session_id=id_k6 and k=6\n",
      ">> No old messages to update summary with\n",
      "---\n",
      "Message 4\n",
      "---\n",
      "\n",
      "get_chat_history called with session_id=id_k6 and k=6\n",
      ">> Found 8 messages, dropping oldest 2 messages.\n",
      ">> New summary: The conversation began with the user introducing themselves as James Potter. The AI responded warmly, asking how it could assist. No further exchanges have occurred beyond this initial greeting.\n",
      "---\n",
      "Message 5\n",
      "---\n",
      "\n",
      "get_chat_history called with session_id=id_k6 and k=6\n",
      ">> Found existing summary\n",
      ">> Found 8 messages, dropping oldest 2 messages.\n",
      ">> New summary: The conversation started with the user, James Potter, greeting the AI, which responded warmly but no further exchanges occurred. In the latest interaction, the user inquired about researching the different types of conversational memory. The AI offered to provide either an overview of the main types or details about specific types of conversational memory, indicating readiness to assist further.\n",
      "---\n",
      "Message 6\n",
      "---\n",
      "\n",
      "get_chat_history called with session_id=id_k6 and k=6\n",
      ">> Found existing summary\n",
      ">> Found 8 messages, dropping oldest 2 messages.\n",
      ">> New summary: The conversation began with the user, James Potter, greeting the AI, but no further exchanges occurred until recently. In the latest interaction, James mentioned exploring ConversationBufferMemory and ConversationBufferWindowMemory. The AI explained that ConversationBufferMemory stores the entire conversation history, whereas ConversationBufferWindowMemory retains only the most recent exchanges within a specified window size. The AI offered to provide more details or discuss their use cases, indicating readiness to assist further.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='The conversation began with the user, James Potter, greeting the AI, but no further exchanges occurred until recently. In the latest interaction, James mentioned exploring ConversationBufferMemory and ConversationBufferWindowMemory. The AI explained that ConversationBufferMemory stores the entire conversation history, whereas ConversationBufferWindowMemory retains only the most recent exchanges within a specified window size. The AI offered to provide more details or discuss their use cases, indicating readiness to assist further.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer memory just stores the entire conversation', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Yes, ConversationBufferMemory stores the entire conversation history. Would you like to know how ConversationBufferWindowMemory differs or how to implement these?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 284, 'total_tokens': 312, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_1a97b5aa6c', 'finish_reason': 'stop', 'logprobs': None}, id='run--f08e0530-5501-453f-8554-fa4b6f2efac8-0', usage_metadata={'input_tokens': 284, 'output_tokens': 28, 'total_tokens': 312, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Correct, ConversationBufferWindowMemory retains only the last k messages, discarding earlier parts of the conversation. Would you like to compare their advantages or see examples of each?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 343, 'total_tokens': 377, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_1a97b5aa6c', 'finish_reason': 'stop', 'logprobs': None}, id='run--a47988f4-9f08-439c-99e7-4dafba48e3e1-0', usage_metadata={'input_tokens': 343, 'output_tokens': 34, 'total_tokens': 377, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " HumanMessage(content='What is my name again?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Your name is James Potter.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 391, 'total_tokens': 397, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_1a97b5aa6c', 'finish_reason': 'stop', 'logprobs': None}, id='run--794b4227-ed00-4b8a-9534-f0cfccaa137f-0', usage_metadata={'input_tokens': 391, 'output_tokens': 6, 'total_tokens': 397, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chat_map[\"id_k6\"].clear()  # clear the history\n",
    "\n",
    "for i, msg in enumerate([\n",
    "    \"Hi, my name is James Potter\",\n",
    "    \"I'm researching the different types of conversational memory.\",\n",
    "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
    "    \"Buffer memory just stores the entire conversation\",\n",
    "    \"Buffer window memory stores the last k messages, dropping the rest.\",\n",
    "    \"What is my name again?\"\n",
    "]):\n",
    "    print(f\"---\\nMessage {i+1}\\n---\\n\")\n",
    "    pipeline_with_history.invoke(\n",
    "        {\"query\": msg},\n",
    "        config={\"session_id\": \"id_k6\", \"llm\": llm, \"k\": 6}\n",
    "    )\n",
    "\n",
    "chat_map[\"id_k6\"].messages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep_Learning_V2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
